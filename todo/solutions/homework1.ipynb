{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Homework 1 - Berkeley STAT 157\n",
    "\n",
    "1. Write all code in the notebook.\n",
    "1. Write all text in the notebook. You can use MathJax to insert math or generic Markdown to insert figures (it's unlikely you'll need the latter). \n",
    "1. **Execute** the notebook and **save** the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-22T19:57:47.188990Z",
     "start_time": "2019-01-22T19:57:46.107420Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Speedtest for vectorization\n",
    "\n",
    "Your goal is to measure the speed of linear algebra operations for different levels of vectorization.  \n",
    "\n",
    "1. Construct two matrices $A$ and $B$ with Gaussian random entries of size $4096 \\times 4096$. \n",
    "1. Compute $C = A B$ using matrix-matrix operations and report the time. \n",
    "1. Compute $C = A B$, treating $A$ as a matrix but computing the result for each column of $B$ one at a time. Report the time.\n",
    "1. Compute $C = A B$, treating $A$ and $B$ as collections of vectors. Report the time.\n",
    "1. Bonus question - what changes if you execute this on a GPU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.randn(4096, 4096)\n",
    "B = torch.randn(4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.83 s, sys: 158 ms, total: 3.98 s\n",
      "Wall time: 561 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C = torch.zeros(4096, 4096)\n",
    "C = A.mm(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46.9 s, sys: 96.5 ms, total: 47 s\n",
      "Wall time: 4.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "C = torch.zeros(4096, 4096)\n",
    "for col in range(4096):\n",
    "    C[:, col] = A.mm(B[:, col].view(-1, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torchime\n",
    "C = torch.zeros(4096, 4096)\n",
    "for row in range(4096):\n",
    "    for col in range(4096):\n",
    "        C[row, col] = torch.mm(A[row, :].view(1, -1), B[:, col].view(-1, 1)).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semidefinite Matrices\n",
    "\n",
    "Assume that $A \\in \\mathbb{R}^{m \\times n}$ is an arbitrary matrix and that $D \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with nonnegative entries. \n",
    "\n",
    "1. Prove that $B = A D A^\\top$ is a positive semidefinite matrix. \n",
    "1. When would it be useful to work with $B$ and when is it better to use $A$ and $D$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 对任意非零实向量$X \\in \\mathbb{R}^{m \\times 1}$,\n",
    "$$X^T B X = X^T A D A^T X = (A^T X)^T D (A^T X)$$\n",
    "令$y=A^T X \\in \\mathbb{R}^{n \\times 1}$,则\n",
    "$$(A^T X)^T D (A^T X) = y^T D y = \\sum^n_{i=1}(y_i^2 d_i) \\ge 0$$\n",
    "\n",
    "2. 假设$C \\in \\mathbb{R}^{m \\times c}$, 则$BC$的算法复杂度为$O(m^2c)$,$ADA^TC$的算法复杂度为$O(2mnk+n^2k)$(这个复杂度是怎么算出来的???)。  \n",
    "    当$m >> n$时，使用$A$和$D$。当$m << n$时，使用$B$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PyTorch on GPUs\n",
    "\n",
    "1. Install GPU drivers (if needed)\n",
    "1. Install PyTorch on a GPU instance\n",
    "1. Display `!nvidia-smi`\n",
    "1. Create a $2 \\times 2$ matrix on the GPU and print it. See [use-gpu](../../chapter_deep-learning-computation/use-gpu.ipynb) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  7 15:17:25 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:B3:00.0 Off |                  N/A |\n",
      "| 16%   24C    P8     1W / 250W |     26MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      1577      G   /usr/lib/xorg/Xorg                             9MiB |\n",
      "|    0      1612      G   /usr/bin/gnome-shell                          14MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = torch.ones(2, 2).to('cuda')\n",
    "D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tensor and NumPy \n",
    "\n",
    "Your goal is to measure the speed penalty between PyTorch and Python when converting data between both. We are going to do this as follows:\n",
    "\n",
    "1. Create two Gaussian random matrices $A, B$ of size $4096 \\times 4096$ in Tensor. \n",
    "1. Compute a vector $\\mathbf{c} \\in \\mathbb{R}^{4096}$ where $c_i = \\|A B_{i\\cdot}\\|^2$ where $\\mathbf{c}$ is a **NumPy** vector.\n",
    "\n",
    "To see the difference in speed due to Python perform the following two experiments and measure the time:\n",
    "\n",
    "1. Compute $\\|A B_{i\\cdot}\\|^2$ one at a time and assign its outcome to $\\mathbf{c}_i$ directly.\n",
    "1. Use an intermediate storage vector $\\mathbf{d}$ in Tensor for assignments and copy to NumPy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = torch.randn(4096, 4096)\n",
    "B = torch.randn(4096, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 0 ns, total: 16 µs\n",
      "Wall time: 23.4 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "c = np.zeros(4096)\n",
    "for i in range(4096):\n",
    "    c[i] = A.mm(B[:, i].view(-1, 1)).norm().item() ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9 µs, sys: 0 ns, total: 9 µs\n",
      "Wall time: 16.5 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "c = np.zeros(4096)\n",
    "d = torch.zeros(4096)\n",
    "for i in range(4096):\n",
    "    d[i] = A.mm(B[:, i].view(-1, 1)).norm(p=2).item()\n",
    "c = d.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Memory efficient computation\n",
    "\n",
    "We want to compute $C \\leftarrow A \\cdot B + C$, where $A, B$ and $C$ are all matrices. Implement this in the most memory efficient manner. Pay attention to the following two things:\n",
    "\n",
    "1. Do not allocate new memory for the new value of $C$.\n",
    "1. Do not allocate new memory for intermediate results if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -29.1079,   16.9853,  -45.4275,  ...,  -78.9186,   72.0830,\n",
       "           25.4923],\n",
       "        [  34.5757,   21.6534,   61.2509,  ...,    2.7386,  148.0545,\n",
       "           17.4609],\n",
       "        [  74.0823, -125.7534,   93.8716,  ...,  -28.1272,  -10.1727,\n",
       "           -6.8330],\n",
       "        ...,\n",
       "        [  23.3093,  -12.9859, -119.0640,  ...,   71.6363,  146.0223,\n",
       "          -24.5569],\n",
       "        [-130.8839,  -41.2710,   61.3858,  ..., -134.8466,  -25.3129,\n",
       "           16.5158],\n",
       "        [  93.5614,   -3.1910,   57.2624,  ...,  -31.2232, -113.6387,\n",
       "          -13.6262]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mm(A, B, out=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Broadcast Operations\n",
    "\n",
    "In order to perform polynomial fitting we want to compute a design matrix $A$ with \n",
    "\n",
    "$$A_{ij} = x_i^j$$\n",
    "\n",
    "Our goal is to implement this **without a single for loop** entirely using vectorization and broadcast. Here $1 \\leq j \\leq 20$ and $x = \\{-10, -9.9, \\ldots 10\\}$. Implement code that generates such a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000e+01,  1.0000e+02, -1.0000e+03,  ...,  1.0000e+18,\n",
       "         -1.0000e+19,  1.0000e+20],\n",
       "        [-9.9000e+00,  9.8010e+01, -9.7030e+02,  ...,  8.3451e+17,\n",
       "         -8.2617e+18,  8.1791e+19],\n",
       "        [-9.8000e+00,  9.6040e+01, -9.4119e+02,  ...,  6.9514e+17,\n",
       "         -6.8123e+18,  6.6761e+19],\n",
       "        ...,\n",
       "        [ 9.8000e+00,  9.6040e+01,  9.4119e+02,  ...,  6.9514e+17,\n",
       "          6.8123e+18,  6.6761e+19],\n",
       "        [ 9.9000e+00,  9.8010e+01,  9.7030e+02,  ...,  8.3451e+17,\n",
       "          8.2617e+18,  8.1791e+19],\n",
       "        [ 1.0000e+01,  1.0000e+02,  1.0000e+03,  ...,  1.0000e+18,\n",
       "          1.0000e+19,  1.0000e+20]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(-10, 10.1, 0.1).view(-1, 1)\n",
    "j = torch.arange(1, 21, 1).float().view(1, -1)\n",
    "A = x ** j\n",
    "A"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
