{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数的访问、初始化和共享\n",
    "\n",
    "在[“线性回归的简洁实现”](../chapter_deep-learning-basics/linear-regression-gluon.ipynb)一节中，我们通过`init`模块来初始化模型的全部参数。我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。\n",
    "\n",
    "我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们使用torch.nn中的`init`模块，它包含了多种模型初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add_module(\"hidden\", nn.Linear(20, 256))\n",
    "net.add_module(\"activation\", nn.ReLU())\n",
    "net.add_module(\"output\", nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "Y = net(X) # 前向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问模型参数\n",
    "\n",
    "对于使用`Sequential`类构造的神经网络，我们可以通过方括号`[]`来访问网络的任一层。回忆一下上一节中提到的`Sequential`类与`Module`类的继承关系。对于`Sequential`实例中含模型参数的层，我们可以通过`Module`类的`parameters()`或者`named_parameters()`函数来访问该层包含的所有参数。下面，访问多层感知机`net`中隐藏层的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden.weight Parameter containing:\n",
      "tensor([[-1.7980e-01, -1.3801e-01,  3.8452e-02,  ..., -2.2064e-01,\n",
      "         -1.4085e-04,  3.8173e-03],\n",
      "        [ 1.4615e-01, -1.0931e-01, -4.8889e-02,  ..., -1.0573e-01,\n",
      "          2.1557e-01,  1.8497e-01],\n",
      "        [ 2.2294e-01,  2.0518e-01,  1.3632e-01,  ..., -2.4643e-03,\n",
      "         -6.7451e-03, -1.2742e-01],\n",
      "        ...,\n",
      "        [ 5.4992e-02, -2.1382e-01, -2.0951e-01,  ..., -1.4173e-01,\n",
      "          2.1169e-01, -3.8293e-02],\n",
      "        [ 1.9754e-01, -1.9902e-01, -2.3991e-04,  ...,  1.5259e-02,\n",
      "         -2.9124e-02,  1.8906e-01],\n",
      "        [ 1.6487e-01, -1.7934e-01,  1.7543e-01,  ..., -1.2741e-01,\n",
      "          1.8801e-01, -1.5623e-01]], requires_grad=True)\n",
      "hidden.bias Parameter containing:\n",
      "tensor([-0.0175,  0.0716, -0.1499,  0.0138, -0.0296,  0.0573, -0.1959,  0.1478,\n",
      "         0.1131,  0.1252,  0.2031,  0.2067, -0.0287,  0.0044, -0.0165, -0.0141,\n",
      "        -0.2110, -0.0156,  0.2180,  0.0147, -0.0230, -0.0619,  0.1719,  0.0875,\n",
      "        -0.0037, -0.1027, -0.1653,  0.0690, -0.0444, -0.2026, -0.0514,  0.1516,\n",
      "        -0.1770, -0.0339,  0.2189, -0.1012, -0.2067, -0.2179, -0.0785, -0.1122,\n",
      "         0.1451, -0.2156, -0.0332,  0.0769, -0.1787,  0.1205,  0.1172, -0.2129,\n",
      "        -0.0217,  0.0599, -0.2166,  0.1477,  0.0624, -0.0371,  0.1689,  0.1540,\n",
      "         0.0961,  0.2157, -0.1117,  0.1082,  0.0297,  0.0828,  0.0839,  0.0798,\n",
      "         0.2142, -0.0303, -0.1082,  0.1272, -0.1025, -0.0566, -0.1266, -0.1866,\n",
      "         0.1244,  0.0132, -0.1414, -0.2158,  0.0360, -0.0493, -0.0388,  0.1897,\n",
      "         0.1933, -0.0839, -0.0614,  0.0068,  0.0700,  0.1655, -0.2163,  0.1729,\n",
      "         0.0211, -0.0802, -0.0813, -0.2076, -0.0303,  0.1834, -0.2158,  0.0262,\n",
      "         0.1593, -0.1989,  0.1681, -0.2170, -0.0113,  0.0176, -0.1619,  0.0882,\n",
      "         0.0884,  0.0364,  0.2165, -0.1789, -0.0902,  0.0349, -0.0428,  0.1776,\n",
      "        -0.0462,  0.0772, -0.0305, -0.1780,  0.0840, -0.1044,  0.1665, -0.1277,\n",
      "         0.0992,  0.2149,  0.1217, -0.1010, -0.0722, -0.0044,  0.1454, -0.1183,\n",
      "         0.1413, -0.0587,  0.1590, -0.1292, -0.0512,  0.0100, -0.1048,  0.1438,\n",
      "         0.1930, -0.0600, -0.1199,  0.1257,  0.0700,  0.1987, -0.0482, -0.2094,\n",
      "        -0.1540, -0.1007, -0.1104, -0.0923, -0.1216,  0.1278,  0.0846, -0.1346,\n",
      "        -0.0509,  0.1475, -0.2073, -0.1807,  0.0945,  0.0997, -0.1037,  0.0288,\n",
      "        -0.0240, -0.0604,  0.0245,  0.0647, -0.2148, -0.1555,  0.0210,  0.0686,\n",
      "        -0.0888,  0.0641, -0.1348,  0.2087,  0.0667, -0.0128,  0.2137,  0.2134,\n",
      "        -0.0096, -0.0631,  0.0588,  0.0305,  0.0806,  0.2070, -0.1030, -0.0953,\n",
      "        -0.1988,  0.0275, -0.0241,  0.2197, -0.2190, -0.1902,  0.0829, -0.0528,\n",
      "         0.0489, -0.1381,  0.0531,  0.2200, -0.1843, -0.0599,  0.1524, -0.2201,\n",
      "        -0.0188, -0.0715,  0.0581, -0.1739, -0.1842,  0.1115, -0.0340, -0.2023,\n",
      "        -0.0726, -0.2212,  0.1387,  0.2070,  0.1936, -0.0208,  0.1667,  0.0353,\n",
      "        -0.2228, -0.0863, -0.1007,  0.1486,  0.0503, -0.1398,  0.1408, -0.1497,\n",
      "         0.0529,  0.2069, -0.1563, -0.0480, -0.0215,  0.1295,  0.2062, -0.0878,\n",
      "        -0.2133,  0.1890,  0.1432, -0.1262,  0.0841, -0.0456,  0.0497,  0.0977,\n",
      "        -0.1463, -0.2233,  0.0355, -0.1685,  0.0139,  0.2153, -0.0142,  0.1502,\n",
      "        -0.1613, -0.1097, -0.1023, -0.1174,  0.0968,  0.1679,  0.1148,  0.1417],\n",
      "       requires_grad=True)\n",
      "output.weight Parameter containing:\n",
      "tensor([[ 0.0054,  0.0351,  0.0211,  ...,  0.0064, -0.0316, -0.0613],\n",
      "        [ 0.0622, -0.0181, -0.0307,  ...,  0.0060,  0.0523, -0.0471],\n",
      "        [-0.0254, -0.0277, -0.0343,  ..., -0.0310,  0.0511,  0.0327],\n",
      "        ...,\n",
      "        [-0.0347, -0.0477,  0.0266,  ..., -0.0493,  0.0140, -0.0423],\n",
      "        [-0.0072, -0.0217,  0.0478,  ...,  0.0093, -0.0336,  0.0619],\n",
      "        [-0.0442,  0.0360,  0.0147,  ...,  0.0143,  0.0075,  0.0472]],\n",
      "       requires_grad=True)\n",
      "output.bias Parameter containing:\n",
      "tensor([-0.0089, -0.0378,  0.0618,  0.0384,  0.0471, -0.0395, -0.0108, -0.0537,\n",
      "         0.0074, -0.0080], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "type(net.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们得到了一个返回参数名和参数值的迭代器。其中隐藏层权重参数的名称为`hidden.weight`，它由`net[0]`的名称（`hidden`）和自己的变量名（`weight`）组成。\n",
    "\n",
    "\n",
    "*注：如果单独调用`net[0].named_parameters()`获得的权重参数名为`weight`*\n",
    "\n",
    "\n",
    "为了访问特定参数，我们可以使用`net.state_dict()`来获得一个由参数名映射到参数值的字典（类型为`OrderedDict`）。通过名字来访问字典里的元素，也可以直接使用它的变量名。下面两种方法是等价的，但通常后者的代码可读性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.7980e-01, -1.3801e-01,  3.8452e-02,  ..., -2.2064e-01,\n",
       "          -1.4085e-04,  3.8173e-03],\n",
       "         [ 1.4615e-01, -1.0931e-01, -4.8889e-02,  ..., -1.0573e-01,\n",
       "           2.1557e-01,  1.8497e-01],\n",
       "         [ 2.2294e-01,  2.0518e-01,  1.3632e-01,  ..., -2.4643e-03,\n",
       "          -6.7451e-03, -1.2742e-01],\n",
       "         ...,\n",
       "         [ 5.4992e-02, -2.1382e-01, -2.0951e-01,  ..., -1.4173e-01,\n",
       "           2.1169e-01, -3.8293e-02],\n",
       "         [ 1.9754e-01, -1.9902e-01, -2.3991e-04,  ...,  1.5259e-02,\n",
       "          -2.9124e-02,  1.8906e-01],\n",
       "         [ 1.6487e-01, -1.7934e-01,  1.7543e-01,  ..., -1.2741e-01,\n",
       "           1.8801e-01, -1.5623e-01]]), Parameter containing:\n",
       " tensor([[-1.7980e-01, -1.3801e-01,  3.8452e-02,  ..., -2.2064e-01,\n",
       "          -1.4085e-04,  3.8173e-03],\n",
       "         [ 1.4615e-01, -1.0931e-01, -4.8889e-02,  ..., -1.0573e-01,\n",
       "           2.1557e-01,  1.8497e-01],\n",
       "         [ 2.2294e-01,  2.0518e-01,  1.3632e-01,  ..., -2.4643e-03,\n",
       "          -6.7451e-03, -1.2742e-01],\n",
       "         ...,\n",
       "         [ 5.4992e-02, -2.1382e-01, -2.0951e-01,  ..., -1.4173e-01,\n",
       "           2.1169e-01, -3.8293e-02],\n",
       "         [ 1.9754e-01, -1.9902e-01, -2.3991e-04,  ...,  1.5259e-02,\n",
       "          -2.9124e-02,  1.8906e-01],\n",
       "         [ 1.6487e-01, -1.7934e-01,  1.7543e-01,  ..., -1.2741e-01,\n",
       "           1.8801e-01, -1.5623e-01]], requires_grad=True))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()['weight'], net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重梯度的形状和权重的形状一样。因为我们还没有进行反向传播计算，所以梯度为None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们可以访问其他层的参数，如输出层的偏差值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0089, -0.0378,  0.0618,  0.0384,  0.0471, -0.0395, -0.0108, -0.0537,\n",
       "         0.0074, -0.0080], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "我们在[“数值稳定性和模型初始化”](../chapter_deep-learning-basics/numerical-stability-and-init.ipynb)一节中描述了模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数则全为0。但我们经常需要使用其他方法来初始化权重。PyTorch在`nn.init`模块里提供了多种预设的初始化单个参数的方法。在下面的例子中，我们将隐藏层的权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并将偏差参数初始化为常数0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0048, -0.0145, -0.0143, -0.0025,  0.0096,  0.0049, -0.0030, -0.0116,\n",
       "         -0.0154,  0.0088,  0.0069, -0.0130,  0.0047, -0.0067, -0.0085,  0.0036,\n",
       "          0.0024,  0.0031, -0.0096,  0.0108]), tensor(0.))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(net[0].weight, mean=0, std=0.01)  # weight\n",
    "nn.init.constant_(net[0].bias, 0) # bias\n",
    "\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义初始化方法\n",
    "\n",
    "如果需要将模型中所有网络层的参数按照相同的策略进行初始化，可以自定义一个初始化方法（如`weight_init`），然后使用`.apply(weight_init)`进行自定义初始化。在下面的例子里，我们令Linear层的权重有一半概率初始化为0，有另一半概率初始化为$[-10,-5]$和$[5,10]$两个区间里均匀分布的随机数。\n",
    "\n",
    "\n",
    "参考自：<https://discuss.pytorch.org/t/reset-the-parameters-of-a-model/29839>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init torch.Size([256, 20])\n",
      "Init torch.Size([10, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000,  9.4008, -0.0000,  5.9762,  9.4529,  0.0000,  5.9044, -0.0000,\n",
       "        -0.0000,  0.0000, -0.0000,  5.8048, -0.0000,  0.0000, -6.9679, -0.0000,\n",
       "        -0.0000, -5.8647,  5.1976, -7.0053])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch不允许对requires_gead=True的tensor做inplace操作。\n",
    "# 所以需要取出weight的 data 属性，对它进行相应的处理\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print('Init', m.weight.shape)\n",
    "        m.weight.data.uniform_(-10, to=10)\n",
    "        m.weight.data *= (m.weight.data.abs() >= 5).float()\n",
    "\n",
    "net.apply(weight_init)\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，我们还可以通过`Parameter`类的`data`属性来直接改写模型参数。例如，在下例中我们将隐藏层参数在现有的基础上加1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000, 10.4008,  1.0000,  6.9762, 10.4529,  1.0000,  6.9044,  1.0000,\n",
       "         1.0000,  1.0000,  1.0000,  6.8048,  1.0000,  1.0000, -5.9679,  1.0000,\n",
       "         1.0000, -4.8647,  6.1976, -6.0053])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data = net[0].weight.data + 1\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享模型参数\n",
    "\n",
    "在有些情况下，我们希望在多个层之间共享模型参数。[“模型构造”](model-construction.ipynb)一节介绍了如何在`Module`类的`forward`函数里多次调用同一个层来计算。\n",
    "\n",
    "*注：经过查找，PyTorch并没有提供其他的共享参数并且传播时保持梯度的方法。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 有多种方法来访问、初始化和共享模型参数。\n",
    "* 可以自定义初始化方法。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 查阅有关`nn.init`模块的PyTorch文档，了解不同的参数初始化方法。\n",
    "* 尝试在`net`实例化后、`net(X)`前访问模型参数，观察模型参数的形状。\n",
    "* 构造一个含共享参数层的多层感知机并训练。在训练过程中，观察每一层的模型参数和梯度。\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/987)\n",
    "\n",
    "![](../img/qr_parameters.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
