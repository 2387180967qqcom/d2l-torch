{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数的访问、初始化和共享\n",
    "\n",
    "在[“线性回归的简洁实现”](../chapter_deep-learning-basics/linear-regression-gluon.ipynb)一节中，我们通过`init`模块来初始化模型的全部参数。我们也介绍了访问模型参数的简单方法。本节将深入讲解如何访问和初始化模型参数，以及如何在多个层之间共享同一份模型参数。\n",
    "\n",
    "我们先定义一个与上一节中相同的含单隐藏层的多层感知机。我们依然使用默认方式初始化它的参数，并做一次前向计算。与之前不同的是，在这里我们使用torch.nn中的`init`模块，它包含了多种模型初始化方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add_module(\"hidden\", nn.Linear(20, 256))\n",
    "net.add_module(\"activation\", nn.ReLU())\n",
    "net.add_module(\"output\", nn.Linear(256, 10))\n",
    "\n",
    "X = torch.rand(2, 20)\n",
    "Y = net(X) # 前向计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 访问模型参数\n",
    "\n",
    "对于使用`Sequential`类构造的神经网络，我们可以通过方括号`[]`来访问网络的任一层。回忆一下上一节中提到的`Sequential`类与`Module`类的继承关系。对于`Sequential`实例中含模型参数的层，我们可以通过`Module`类的`parameters()`或者`named_parameters()`函数来访问该层包含的所有参数。下面，访问多层感知机`net`中隐藏层的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden.weight Parameter containing:\n",
      "tensor([[-0.0949, -0.0619,  0.0468,  ...,  0.0801,  0.0061,  0.2128],\n",
      "        [ 0.1147,  0.2220, -0.0054,  ..., -0.0149, -0.1584,  0.1572],\n",
      "        [ 0.0741, -0.1227, -0.0314,  ...,  0.0975,  0.1925,  0.0592],\n",
      "        ...,\n",
      "        [-0.0919, -0.0317, -0.0594,  ..., -0.1490,  0.0025, -0.1462],\n",
      "        [-0.0113, -0.0613, -0.0177,  ...,  0.0515, -0.0154,  0.2067],\n",
      "        [-0.0547, -0.0829, -0.1156,  ..., -0.1708, -0.1700,  0.0933]],\n",
      "       requires_grad=True)\n",
      "hidden.bias Parameter containing:\n",
      "tensor([ 1.0377e-01, -3.7003e-02, -8.5052e-02,  2.0793e-01,  1.1988e-01,\n",
      "        -2.2192e-01,  1.9521e-01, -4.9697e-03,  5.9934e-02, -1.4455e-01,\n",
      "        -1.6245e-01,  1.8136e-01, -2.1201e-01, -5.1087e-02, -3.1404e-02,\n",
      "        -6.4125e-02, -1.1353e-01,  9.8943e-02, -1.9422e-01,  8.7757e-02,\n",
      "        -1.7677e-01,  1.3115e-01, -6.6839e-02, -1.1996e-01,  1.3070e-01,\n",
      "        -1.3522e-01,  2.2208e-01, -6.1894e-02,  6.3665e-02, -1.4802e-01,\n",
      "        -1.5915e-02, -2.2043e-01,  2.0974e-01,  1.6393e-01,  1.3937e-01,\n",
      "         5.0195e-02, -1.3864e-01,  1.2289e-01, -1.6591e-01,  1.0961e-01,\n",
      "        -1.1053e-01, -1.0894e-01, -1.9171e-01,  1.1770e-01, -1.6427e-01,\n",
      "        -5.5441e-02,  1.1427e-01, -7.4085e-02,  1.5365e-01,  2.8777e-02,\n",
      "        -1.5103e-01,  1.9134e-01, -1.5904e-01, -2.1830e-01,  1.8212e-02,\n",
      "         3.5763e-02,  1.3833e-01,  7.2853e-02,  7.1351e-02, -1.7545e-01,\n",
      "         1.6373e-01,  1.7055e-01,  1.1215e-01,  4.6957e-02,  1.4797e-01,\n",
      "        -1.0471e-01,  1.2717e-01, -1.4198e-01,  1.8637e-01, -6.1607e-02,\n",
      "        -2.1991e-01,  1.6576e-01, -1.3779e-01, -1.9249e-01, -1.1218e-02,\n",
      "        -2.8395e-02,  1.3782e-02, -1.9613e-01, -1.0688e-01, -1.4914e-01,\n",
      "        -3.8109e-02, -1.3459e-01,  1.3687e-01,  1.2884e-01, -2.1039e-01,\n",
      "        -1.3152e-01,  7.6935e-02,  1.2902e-01, -1.7735e-01, -1.2342e-01,\n",
      "        -1.6805e-01,  4.1420e-02, -8.3830e-02, -7.5987e-03,  1.8309e-01,\n",
      "         1.5174e-01,  7.1289e-02, -2.0061e-01,  1.2927e-01,  2.1636e-01,\n",
      "        -1.7884e-01, -1.2065e-01,  1.8807e-01, -1.5673e-01, -2.6594e-02,\n",
      "        -1.6566e-01,  9.4070e-02, -4.8738e-02, -1.1972e-01, -4.1545e-02,\n",
      "        -5.7861e-02,  2.0757e-01, -2.0333e-02,  4.0361e-02, -1.1453e-02,\n",
      "        -1.4005e-01, -1.3548e-01,  1.5804e-01,  1.6909e-01, -7.0026e-02,\n",
      "        -1.7921e-01, -7.5300e-02, -1.2077e-01,  2.1223e-01,  3.0509e-02,\n",
      "         1.4348e-01, -1.4840e-01,  2.9068e-02,  8.8635e-02, -1.9161e-01,\n",
      "         1.2781e-01, -1.1748e-01, -9.9720e-02, -7.4686e-02,  1.7678e-01,\n",
      "        -1.6382e-01,  1.2286e-01, -1.6909e-01, -1.7308e-01,  1.6650e-01,\n",
      "        -1.1825e-01, -9.0432e-02,  5.5573e-02, -1.6372e-01, -1.9994e-02,\n",
      "         5.5461e-02,  1.1066e-01, -6.9028e-02, -1.5376e-01, -1.8257e-01,\n",
      "        -1.5158e-02,  2.2493e-02, -3.6876e-02, -1.9341e-01,  1.8249e-01,\n",
      "         6.9616e-02, -2.1983e-01,  1.0638e-01, -1.0608e-01, -1.3831e-01,\n",
      "         1.6177e-01, -1.2520e-01, -6.5965e-02,  1.5125e-01, -8.7622e-02,\n",
      "         1.6482e-01,  1.7542e-01,  1.0807e-01, -1.4692e-01, -2.1937e-01,\n",
      "        -1.7089e-02, -1.4167e-01, -9.9721e-02, -2.1912e-01,  1.7010e-01,\n",
      "        -7.5870e-02, -1.7731e-01,  1.3880e-01, -3.8442e-02, -1.9022e-01,\n",
      "         2.0704e-04,  1.3185e-01,  1.3959e-01, -1.6047e-01,  1.7491e-01,\n",
      "         9.8341e-03, -1.3605e-01, -2.0106e-01, -1.8862e-02, -1.0678e-01,\n",
      "         1.3046e-01, -2.9133e-03,  1.2824e-01, -1.8671e-01,  2.1359e-01,\n",
      "         1.9434e-01,  4.1841e-02,  1.1456e-02,  2.1318e-01, -9.7987e-02,\n",
      "         1.0084e-01,  1.3405e-01, -6.0296e-02, -1.5886e-01,  1.9506e-01,\n",
      "         8.9689e-02, -2.7756e-03, -1.6109e-01, -1.1335e-01,  2.0966e-02,\n",
      "         1.9703e-02, -1.9031e-02, -2.7307e-02,  1.9257e-01,  1.5754e-01,\n",
      "        -1.0088e-01,  3.2839e-02, -6.9227e-02, -7.7774e-02, -1.0658e-01,\n",
      "        -7.4658e-02, -1.1848e-01,  7.7932e-02, -3.8609e-02, -1.1566e-01,\n",
      "        -1.5850e-01, -1.1885e-01, -1.8678e-01, -1.1322e-01, -6.7213e-02,\n",
      "         6.6543e-02,  1.8623e-01, -9.6973e-02, -5.7903e-02, -4.4721e-02,\n",
      "         2.2384e-02,  7.7721e-02,  9.4091e-02, -1.4802e-01,  3.8284e-02,\n",
      "         7.8768e-02, -5.8623e-02,  6.3828e-02,  2.0305e-01,  1.5502e-02,\n",
      "        -1.6954e-01,  1.1456e-01,  1.0314e-01,  1.2512e-02, -3.0060e-03,\n",
      "        -1.2331e-01, -4.0848e-02,  1.4407e-02, -1.8377e-01, -6.8135e-02,\n",
      "        -1.8775e-02], requires_grad=True)\n",
      "output.weight Parameter containing:\n",
      "tensor([[ 0.0114, -0.0570, -0.0532,  ...,  0.0446, -0.0404, -0.0047],\n",
      "        [ 0.0080, -0.0211,  0.0404,  ...,  0.0497,  0.0320, -0.0155],\n",
      "        [ 0.0362, -0.0535, -0.0392,  ..., -0.0471, -0.0118, -0.0465],\n",
      "        ...,\n",
      "        [-0.0364,  0.0108, -0.0275,  ...,  0.0348,  0.0074,  0.0392],\n",
      "        [ 0.0042,  0.0526,  0.0484,  ..., -0.0603,  0.0102, -0.0313],\n",
      "        [ 0.0333, -0.0265,  0.0065,  ..., -0.0329, -0.0436, -0.0103]],\n",
      "       requires_grad=True)\n",
      "output.bias Parameter containing:\n",
      "tensor([-0.0379, -0.0311,  0.0163,  0.0259,  0.0206, -0.0063, -0.0559, -0.0193,\n",
      "         0.0398,  0.0052], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param)\n",
    "    \n",
    "type(net.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们得到了一个返回参数名和参数值的迭代器。其中隐藏层权重参数的名称为`hidden.weight`，它由`net[0]`的名称（`hidden`）和自己的变量名（`weight`）组成。\n",
    "\n",
    "\n",
    "*注：如果单独调用`net[0].named_parameters()`获得的权重参数名为`weight`*\n",
    "\n",
    "\n",
    "为了访问特定参数，我们可以使用`net.state_dict()`来获得一个由参数名映射到参数值的字典（类型为`OrderedDict`）。通过名字来访问字典里的元素，也可以直接使用它的变量名。下面两种方法是等价的，但通常后者的代码可读性更好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0949, -0.0619,  0.0468,  ...,  0.0801,  0.0061,  0.2128],\n",
       "         [ 0.1147,  0.2220, -0.0054,  ..., -0.0149, -0.1584,  0.1572],\n",
       "         [ 0.0741, -0.1227, -0.0314,  ...,  0.0975,  0.1925,  0.0592],\n",
       "         ...,\n",
       "         [-0.0919, -0.0317, -0.0594,  ..., -0.1490,  0.0025, -0.1462],\n",
       "         [-0.0113, -0.0613, -0.0177,  ...,  0.0515, -0.0154,  0.2067],\n",
       "         [-0.0547, -0.0829, -0.1156,  ..., -0.1708, -0.1700,  0.0933]]),\n",
       " Parameter containing:\n",
       " tensor([[-0.0949, -0.0619,  0.0468,  ...,  0.0801,  0.0061,  0.2128],\n",
       "         [ 0.1147,  0.2220, -0.0054,  ..., -0.0149, -0.1584,  0.1572],\n",
       "         [ 0.0741, -0.1227, -0.0314,  ...,  0.0975,  0.1925,  0.0592],\n",
       "         ...,\n",
       "         [-0.0919, -0.0317, -0.0594,  ..., -0.1490,  0.0025, -0.1462],\n",
       "         [-0.0113, -0.0613, -0.0177,  ...,  0.0515, -0.0154,  0.2067],\n",
       "         [-0.0547, -0.0829, -0.1156,  ..., -0.1708, -0.1700,  0.0933]],\n",
       "        requires_grad=True))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].state_dict()['weight'], net[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重梯度的形状和权重的形状一样。因为我们还没有进行反向传播计算，所以梯度为None。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.grad == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，我们可以访问其他层的参数，如输出层的偏差值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0379, -0.0311,  0.0163,  0.0259,  0.0206, -0.0063, -0.0559, -0.0193,\n",
       "         0.0398,  0.0052], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型参数\n",
    "\n",
    "我们在[“数值稳定性和模型初始化”](../chapter_deep-learning-basics/numerical-stability-and-init.ipynb)一节中描述了模型的默认初始化方法：权重参数元素为[-0.07, 0.07]之间均匀分布的随机数，偏差参数则全为0。但我们经常需要使用其他方法来初始化权重。PyTorch在`nn.init`模块里提供了多种预设的初始化单个参数的方法。在下面的例子中，我们将隐藏层的权重参数初始化成均值为0、标准差为0.01的正态分布随机数，并将偏差参数初始化为常数0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 8.1788e-03, -2.2569e-05,  1.8563e-02,  4.3964e-04, -6.7969e-03,\n",
       "         -2.7120e-02, -1.8878e-02, -6.7642e-03, -1.7379e-03, -5.9111e-03,\n",
       "          6.1728e-03,  2.4607e-03,  1.6794e-02,  4.0205e-03,  2.8080e-02,\n",
       "          2.5098e-03, -1.9734e-02,  1.0123e-02,  1.6951e-02, -1.0615e-02]),\n",
       " tensor(0.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.init.normal_(net[0].weight, mean=0, std=0.01)  # weight\n",
    "nn.init.constant_(net[0].bias, 0) # bias\n",
    "\n",
    "net[0].weight.data[0], net[0].bias.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义初始化方法\n",
    "\n",
    "如果需要将模型中所有网络层的参数按照相同的策略进行初始化，可以自定义一个初始化方法（如`weight_init`），然后使用`.apply(weight_init)`进行自定义初始化。在下面的例子里，我们令Linear层的权重有一半概率初始化为0，有另一半概率初始化为$[-10,-5]$和$[5,10]$两个区间里均匀分布的随机数。\n",
    "\n",
    "\n",
    "参考自：<https://discuss.pytorch.org/t/reset-the-parameters-of-a-model/29839>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init torch.Size([256, 20])\n",
      "Init torch.Size([10, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.0000, -0.0000,  0.0000, -8.4261,  8.0859,  8.2111, -0.0000, -9.6193,\n",
       "        -6.4062, -0.0000,  7.2762,  0.0000, -0.0000,  9.1960,  0.0000,  0.0000,\n",
       "         0.0000, -5.0190,  0.0000,  9.5519])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PyTorch不允许对requires_gead=True的tensor做inplace操作。\n",
    "# 所以需要取出weight的 data 属性，对它进行相应的处理\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        print('Init', m.weight.shape)\n",
    "        m.weight.data.uniform_(-10, to=10)\n",
    "        m.weight.data *= (m.weight.data.abs() >= 5).float()\n",
    "\n",
    "net.apply(weight_init)\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*注：因为 PyTorch 并没有提供对模型所有参数进行初始化的方法，我们写一个自定义的全局初始化参数的方法加入到 d2ltorch 包中。(该方法来自 [Weights-Initializer-pytorch](https://github.com/3ammor/Weights-Initializer-pytorch) )*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_init(model, init, **kwargs): # 本方法已保存在d2ltorch包中方便以后使用\n",
    "\n",
    "    def initialize(m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init(m.weight.data, **kwargs)\n",
    "            try:\n",
    "                init(m.bias.data)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init(m.weight.data, **kwargs)\n",
    "            try:\n",
    "                init(m.bias.data)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1.0)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm1d):\n",
    "            m.weight.data.fill_(1.0)\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "    model.apply(initialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，我们还可以通过`Parameter`类的`data`属性来直接改写模型参数。例如，在下例中我们将隐藏层参数在现有的基础上加1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  1.0000,  1.0000, -7.4261,  9.0859,  9.2111,  1.0000, -8.6193,\n",
       "        -5.4062,  1.0000,  8.2762,  1.0000,  1.0000, 10.1960,  1.0000,  1.0000,\n",
       "         1.0000, -4.0190,  1.0000, 10.5519])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data = net[0].weight.data + 1\n",
    "net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享模型参数\n",
    "\n",
    "在有些情况下，我们希望在多个层之间共享模型参数。[“模型构造”](model-construction.ipynb)一节介绍了如何在`Module`类的`forward`函数里多次调用同一个层来计算。\n",
    "\n",
    "*注：经过查找，PyTorch并没有提供其他的共享参数并且传播时保持梯度的方法。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "* 有多种方法来访问、初始化和共享模型参数。\n",
    "* 可以自定义初始化方法。\n",
    "\n",
    "\n",
    "## 练习\n",
    "\n",
    "* 查阅有关`nn.init`模块的PyTorch文档，了解不同的参数初始化方法。\n",
    "* 尝试在`net`实例化后、`net(X)`前访问模型参数，观察模型参数的形状。\n",
    "* 构造一个含共享参数层的多层感知机并训练。在训练过程中，观察每一层的模型参数和梯度。\n",
    "\n",
    "\n",
    "\n",
    "## 扫码直达[讨论区](https://discuss.gluon.ai/t/topic/987)\n",
    "\n",
    "![](../img/qr_parameters.svg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
